{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum on CPU: 40969262, Execution time on CPU: 0.6395096778869629 seconds\n",
      "Sum on GPU: 40969262, Execution time on GPU: 0.020003557205200195 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "import time\n",
    "\n",
    "# CUDA code for summing vector elements\n",
    "vector_sum_kernel = \"\"\"\n",
    "__global__ void addKernel(int* result, int* a, unsigned int size) {\n",
    "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    // Variable for local sum in block\n",
    "    __shared__ int sharedSum[256];  // block size (can be changed)\n",
    "\n",
    "    int localSum = 0;\n",
    "\n",
    "    // Sum elements\n",
    "    if (index < size) {\n",
    "        localSum = a[index];\n",
    "    }\n",
    "\n",
    "    // Write local sum to shared memory\n",
    "    sharedSum[threadIdx.x] = localSum;\n",
    "    __syncthreads();  // Thread synchronization\n",
    "\n",
    "    // Perform reduction in shared memory\n",
    "    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n",
    "        if (threadIdx.x < stride) {\n",
    "            sharedSum[threadIdx.x] += sharedSum[threadIdx.x + stride];\n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    // Write block result to global memory\n",
    "    if (threadIdx.x == 0) {\n",
    "        atomicAdd(result, sharedSum[0]);  // Use atomic for safe result increment\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Function for summing vector elements on GPU using CUDA\n",
    "def vector_sum_gpu(vector):\n",
    "    '''\n",
    "    # Function for summing vector elements on GPU\n",
    "    :param: vector - input vector\n",
    "    :return: [\n",
    "      answer - multiplication result,\n",
    "    ]\n",
    "    '''\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Allocate memory on GPU\n",
    "    vector_gpu = cuda.mem_alloc(vector.nbytes)\n",
    "    result_gpu = cuda.mem_alloc(np.int32().nbytes)  # Memory for result\n",
    "    initial_value = np.array([0], dtype=np.int32)\n",
    "\n",
    "    # Initialize result\n",
    "    cuda.memcpy_htod(result_gpu, initial_value)\n",
    "\n",
    "    # Copy data to GPU\n",
    "    cuda.memcpy_htod(vector_gpu, vector)\n",
    "\n",
    "    # Compile and load CUDA code\n",
    "    mod = SourceModule(vector_sum_kernel)\n",
    "    vector_sum = mod.get_function(\"addKernel\")\n",
    "\n",
    "    # Define block and grid sizes for parallelization\n",
    "    block_size = 256\n",
    "    grid_size = (len(vector) + block_size - 1) // block_size\n",
    "\n",
    "    # Launch kernel on GPU\n",
    "    vector_sum(result_gpu, vector_gpu, np.int32(len(vector)), block=(block_size, 1, 1), grid=(grid_size, 1))\n",
    "\n",
    "    # Copy result from GPU to CPU\n",
    "    result = np.empty(1, dtype=np.int32)\n",
    "    cuda.memcpy_dtoh(result, result_gpu)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    return result[0], end_time - start_time\n",
    "\n",
    "\n",
    "def vector_sum_cpu(vector):\n",
    "    '''\n",
    "    Function for summing vector elements on CPU\n",
    "    :param: vector - input vector\n",
    "    :return: [\n",
    "      answer - multiplication result,\n",
    "    ]\n",
    "    '''\n",
    "    answer = 0\n",
    "    for elem in vector:\n",
    "        answer += elem\n",
    "    return answer\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate vector with random values\n",
    "    vector_size = 8194000\n",
    "    vector = np.random.randint(1, 10, size=vector_size, dtype=np.int32)\n",
    "\n",
    "    # Sum on CPU\n",
    "    start_time_cpu = time.time()\n",
    "    answer_cpu = vector_sum_cpu(vector)\n",
    "    end_time_cpu = time.time()\n",
    "\n",
    "    time_cpu = end_time_cpu - start_time_cpu\n",
    "    print(f\"Sum on CPU: {answer_cpu}, Execution time on CPU: {time_cpu} seconds\")\n",
    "\n",
    "    # Sum on GPU\n",
    "    answer_gpu, time_gpu = vector_sum_gpu(vector)\n",
    "    print(f\"Sum on GPU: {answer_gpu}, Execution time on GPU: {time_gpu} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
